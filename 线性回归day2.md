# 线性回归

~~~markdown
size m^2		price万
50				500 y^=500  预测值和真实值的偏差=0
90				900
150				1500
200				2100
250				2600
300				？3000 3600
350				4000
400				6000
500				10000
~~~

## 思路的产生

~~~markdown
1. 房价和房子的大小有没有关系？	有
2. 房价和房子的大小有什么关系？	假设存在某种关系
3. 假设存在线性关系。	y^ = ax+b
4. 求a,b a=10,b=0
5. 假设求出了一组a,b，衡量预测值y^和真实值y之间的差距，差距越小越好
6. (y^-y)^2 |y^-y|   选择平方，平方可以放大差距
7. 用这组a,b去衡量整个样本集的差距:f(x)=((y^1-y1)^2+(y^2-y2)^2+...+(y^n-yn)^2)/n
8. 希望平均误差越小越好 求f(x)的最小值
9. 通过计算最小值可以得到对应的a,b
10. 认为这组a,b是所有a,b中最合适的，把a,b带入到刚才假设的线性关系函数中做预测即可
~~~

## 线性回归的开发流程

~~~markdown
1. 提出假设函数
2. 提出损失函数(衡量这组参数对一个样本的误差)
3. 提出代价函数(衡量整个样本集的误差)
4. 计算代价函数的最小值，得到一组a,b
5. 将a,b带入到假设函数做预测即可。

参数
~~~

## 多元线性回归

~~~markdown
size m^2	rooms		price万
50			3			500 y^=500  预测值和真实值的偏差=0
90			5			900
150			5			1500
200			6			2100
250			6			2600
300			6			？3000 3600
350			7			4000
400			7			6000
500			8			10000
~~~

~~~markdown
1. 提出假设函数 y^ = w1x1+w2x2+w3x3+...+wnxn = W * X.T
    W = [w1,w2,w3,...,wn]
    X = [x1,x2,x3,...,xn]
        W(1,n)
        X(1,n)
        W*X.T=w1x1+w2x2+w3x3+...+wnxn
2. 提出损失函数 (W * X.T-y)^2
3. 提出代价函数 f(x)=((W * X1.T-y1)^2+(W * X2.T-y2)^2+...+(W * Xn.T-yn)^2)/n
4. 计算代价函数的最小值
~~~



![梯度下降](D:\TYUT教学资料\TYUT2022\笔记\.assets\梯度下降.png)



### 梯度下降的三种形式

~~~markdown
1. 批量梯度下降：计算导数的时候，每次更新参数W，都需要整个样本集全部带入。准，算的满。
2. 随机梯度下降：计算导数的时候，每次更新参数W，需要一个样本参与。算得快，不准。
3. 小批量梯度下降：计算导数的时候，每次更新参数W，随机使用一部分样本参与。算的相对快，相对准。
~~~

~~~python
pip install scikit-learn
pip install numpy
pip install pandas
~~~

### 拟合

![拟合](D:\TYUT教学资料\TYUT2022\笔记\.assets\拟合.png)



~~~markdown
1. 用一个平滑的曲线来模拟样本的分布情况，称为拟合

做机器学习的目的是为了找到隐藏在数据下的一般性规律

过拟合：拟合的太过了
	1. 特征太多了
	2. 使用了复杂的假设函数
		可以让假设函数的幂小一点，可以让高阶项的参数值小一些
		w1x1+w2x2+...+wnxn
		w1x1^1+w2x2^2+...+wnxn^n
		
	正则化：
		正则项：对参数进行约束的一个式子
		|w| = l1正则项:不用：容易让特征失去意义
		w^2 = l2正则项:用的较多
欠拟合：拟合的太欠了
	1. 因为特征太少了
	2. 增大数据集
	3. 增大特征个数
	4. 使用更复杂的假设函数
~~~

![正则化](D:\TYUT教学资料\TYUT2022\笔记\.assets\正则化.png)



$一般不用！！！：l1正则化代价函数：\sum_{i=1}^{n}(\hat y_i-y_i)^2+\sum_{i=1}^{c}|w_i|,n是样本数,c是特征数$

$l2正则化代价函数:\sum_{i=1}^{n}(\hat y_i-y_i)^2+\sum_{i=1}^{c}{w_i}^2,n是样本数,c是特征数$

### 对线性回归进行改进

~~~markdown
1. 整个样本集都是线性关系的情况，很少
	局部加权线性回归
	在做预测的时候，只考虑距离最近的这些样本点，对它们做一个线性回归
2. 在使用线性回归做二分类的时候，本类之间样本的差异性过大
如果线性回归要做多个分类问题，那么可以拆分为多个二分类问题来解决
~~~

# 逻辑回归

~~~markdown
1. 提出假设函数(在线性回归外套一层sigmoid函数)
2. 提出损失函数(对数损失函数)
3. 提出代价函数(求和损失函数)
4. 求代价函数的最小值，通过梯度下降求解，得到一组参数
~~~

$sigmoid函数：\frac{1}{1+e^{-x}},x指的是线性回归的假设函数(W*X.T)$

$对数损失函数：f(x)=-(ylog_2(\hat y)+(1-y)log_2(1-\hat y))$

$对数代价函数:f(x)=-\sum_{i=1}^{n}(y_ilog_2(\hat y_i)+(1-y_i)log_2(1-\hat y_i))$

### 什么样的函数可以作为损失函数

~~~markdown
1. 损失函数的目的是为了评估真实值和预测值之间的差异，当损失函数的值减小的时候，预测值一定要更接近真实值
均方差损失函数：f(x)=(y^-y)^2
	y=0--->f(x)=(y^)^2->f(x)越小越好->(y^)^2越小越好-> (y^)^2--->0--->y^---0---y
	y=1--->f(x)=(y^-1)^2->f(x)越小越好->(y^-1)^2越小越好->(y^-1)^2--->0--->y^-1--->0--->y^---1---y

对数损失函数可以作为逻辑回归的损失函数，不能作为线性回归的损失函数
~~~

$完整的逻辑回归的代价函数:f(x)=-C\sum_{i=1}^{n}(y_ilog_2(\hat y_i)+(1-y_i)log_2(1-\hat y_i))+\sum_{i=1}^{c}w_i^2$

~~~markdown
通过控制C的数值，来控制优化目标，更希望代价函数项更小还是更希望正则项更小
C的选择：凭经验
~~~

### 使用数学公式作为算法核心的开发流程

~~~markdown
1. 提出假设函数 假设特征矩阵和目标向量存在着一定的关系
2. 提出损失函数 评估一个样本的误差
3. 提出代价函数 评估整个样本集的误差+l2正则项
4. 梯度下降求优化(代价函数最小值)(批量、随机、小批量)
5. 求得代价函数的最小值对应的预测值对应的参数向量W
6. 把W带入到假设函数做预测

注意：梯度下降有可能会出现局部最小值（鞍点）
~~~

# 作业

~~~markdown
1. 自学numpy和pandas的使用方法
2. adult数据集，自己清洗，自己选择算法，自己调整参数，明天看所有人的准确率，高于80%
~~~






























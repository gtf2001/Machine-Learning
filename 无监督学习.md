# 无监督学习

~~~markdown
没有明确的目标向量，只有特征矩阵
~~~

## 聚类

~~~markdown
重新给学生分组
	分成五组：
		1. 先指定五个人做组长，让大家选择跟谁走，选完以后形成了五个组。
		2. 每个组投票选举出一个新的组长，再让大家选择跟谁在一起，形成了五个组。---循环往复
		3. 直到误差小于一定量，或者达到最大迭代次数，就结束
~~~

## Kmeans算法 K均值算法

~~~markdown
核心思想：物以类聚，人以群分

开发流程：
	1. 指定K个组长(质心)
	2. 计算一下每个学生(样本)到这个组长(质心)的距离（欧氏距离、余弦距离）
	3. 将每一个学生(样本)都划分到离他最近的这个组长(质心)的那一组(簇)（形成了一个组(簇)）
	4. 计算每个组(簇)的平均值，将距离平均值最近的那个人(样本)作为新的组长(质心)
	5. 重复2-4步，直到组长(簇)的人选的变化小于一定量或者达到最大迭代次数，不再重复
	6. 输出每个组(簇)
~~~

### DBSCAN基于密度的聚类算法

~~~markdown
1. eps邻域：能够被评判为风云人物的范围
	评判是不是核心点的作用范围
2. 最小点数：是风云人物的前提要求，必须满足中午跟你一起吃饭的人的数量达到这个值
	核心点周围必须出现的样本点的最小的个数
3. 核心点：风云人物
	在eps邻域之内，满足最小点的就是核心点
4. 边界点：充数的龙套，中午跟风云人物一起吃饭的人
	不是核心点，但是是核心点邻域内的点
5. 噪声点：自己吃，不跟风云人物一起吃饭同时也不是风云人物的这种小透明
	既不是核心点，也不是边界点
6. 直接密度可达：对于风云人物而言，在风雨人物周围吃饭的人
	对于核心点而言，所有在邻域内的边界点，都是直接密度可达
7. 间接密度可达：不在风云人物周围，但是在与风云人物可以直接相连的其他的风云人物周围
	对于核心点而言，不在邻域内，但是可以通过其他的核心点寻址过去
8. 密度相连：所有直接密度可达和间接密度可达的人，都称它为与风云人物密度相连
	对于核心点而言，所有的直接密度可达和简介密度可达，都称作密度相连
~~~

#### 开发流程

~~~markdown
1. 指定eps邻域和最小点数
2. 从样本集中选出一个点，判断这个点是否是核心点，不是核心点，放回样本集
3. 是核心点，从这个核心点出发，取出所有与它密度相连的点，这所有的点形成一个簇
4. 一直重复2-3步，直到所有的样本点都被访问过，此时停止
5. 输出每一个簇作为一个类别
~~~

![DBSCAN聚类](D:\TYUT教学资料\TYUT2022\笔记\.assets\DBSCAN聚类.png)



#### DBSCAN优化(调参数)

~~~markdown
1. 聚类的类别变多
	1. 邻域变小
	2. 最小点数增加
2. 聚类的类别变少
	1. 邻域变大
	2. 最小点数减小
~~~

~~~python
'今天天气很好',
'我今天考上了清华大学',
'清华大学很好',
'我今天心情很好',
'清华大学今天让我进去了'
# 1 2 3 2 3
# 1 2 1 1 3
~~~

~~~python
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
text = [
'今天天气很好',
'我今天考上了清华大学',
'清华大学很好',
'我今天心情很好',
'清华大学今天让我进去了'
]
for t in text:
    print(jieba.lcut(t))
tid = TfidfVectorizer(tokenizer=lambda t:jieba.lcut(t))
X = tid.fit_transform(text)
kmeans = KMeans(n_clusters=3)
y = kmeans.fit_predict(X)
print(y)
~~~

~~~python
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
text = [
'今天天气很好',
'我今天考上了清华大学',
'清华大学很好',
'我今天心情很好',
'清华大学今天让我进去了'
]
for t in text:
    print(jieba.lcut(t))
tid = TfidfVectorizer(tokenizer=lambda t:jieba.lcut(t))
X = tid.fit_transform(text)
dbscan = DBSCAN(min_samples=1,eps=1.1)
y = dbscan.fit_predict(X)
print(y)
~~~

### 聚类算法的用途

~~~markdown
对原始数据的划分，使用聚类来代替人为划分
~~~

### 降维算法

~~~markdown
1. 降低数据集的维度，数据集的维度等于数据的特征个数

语文 数学 英语 政治 历史 地理 生物 化学 物理 美术 体育 音乐 计算机 道德与法治 科学       是否聪明
~~~

#### PCA主成分分析

~~~markdown
找到影响最终分类结果的最主要的那些特征，再利用那些特征进行计算
特征值与特征向量：
	比如一个向量a*一个数字m = 一个方阵A*向量a，称向量a是矩阵A的一个特征向量，数字m是对应的特征值
	比如：[1,1] * 2 = [2,2] = [[1,1],[1,1]] * [1,1] 注意：[1,1]是一个列向量
	
	特征值和特征向量的个数与方阵的阶相同
~~~

$方差公式：s^2(var) = \frac{\sum_{i=1}^{n}(x_i-\hat x)^2}{n},为了衡量一个向量的离散程度$

$协方差公式:cov(x,y)=\frac{\sum_{i=1}^{n}(x_i-\hat x)(y_i-\hat y)}{n},用来衡量两个向量之间的相似度$

### PCA的开发流程

~~~markdown
1. 计算特征之间的协方差，并且构建一个协方差矩阵(方阵)
2. 计算协方差矩阵的所有特征值与特征向量
3. 倒序排列所有的特征值，并且依次相加求和，直到累加超过总和的90% K个特征
4. 保留已经累加了的特征值所对应的特征向量，并且构建成一个新矩阵 n * k
5. 将原始矩阵乘以新矩阵得到一个新的特征矩阵。m * k
~~~










































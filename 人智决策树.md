# 决策树

~~~markdown
1. 位置	排数	列数	组别	身高	体重
我的回答只有是或者否

1.位置、排数、列数、组别、身高、体重？每一个问题只能问一遍
袁菲：
    1.是否在前四排？Yes it is
    2.是否在左四列？Yes it is
    3.身高是否在一米七以上？maybe
    4.是否在第二列？No it is’t
    5.是否是范子怡。6
子怡：
    不想玩儿
森浩：
    1.是否在前四行？是
    2.是否在红组？否
    3.是否在一米七以上？是
    4.是否是200斤以上？是
    5.是否是乔鼎？是
    

不同的特征蕴含的信息量不一样的
~~~

![决策树](D:\TYUT教学资料\TYUT2022\笔记\.assets\决策树.png)

## 信息熵

~~~markdown
1. 信息熵：一个系统的混乱程度，系统越混乱，信息熵越大，反之，信息熵越小。
	比如火柴，刚开始很整齐，所以信息熵很小，后来拿出来几根晃一晃，这个时候就变得很混乱，信息熵就很大
	对于我们来说，信息熵越小，越好取值
2. 信息增益：一个系统经过变化之后，信息熵的变化。信息熵增大，信息增益减小，反之亦然。
~~~

$sigmoid函数：\frac{1}{1+e^{-x}}$

$信息熵的计算公式：-\sum_{i=1}^{n}P(C_i)log_2P(C_i)$

| 天气 | 有风 | 温度 | 打球   |
| ---- | ---- | ---- | ------ |
| 晴天 | 无风 | 高温 | 打球   |
| 阴天 | 有风 | 高温 | 不打球 |
| 阴天 | 无风 | 低温 | 打球   |
| 阴天 | 有风 | 一般 | 不打球 |
| 晴天 | 有风 | 低温 | 不打球 |
| 晴天 | 无风 | 一般 | 不打球 |
| 晴天 | 无风 | 低温 | 打球   |

![决策树ID3算法](D:\TYUT教学资料\TYUT2022\笔记\.assets\决策树ID3算法.png)





### ID3算法的弊端

~~~markdown
对于离散性较强的特征的计算有问题，离散性越强，信息熵就越小，但是这种特征包含的信息也是最小的。
~~~

### C4.5算法

~~~~markdown
1. 信息增益率：系统的信息熵与自身的信息熵比值，越大越好，越大证明信息量越多
		系统的信息熵：-(p(打球)log(p(打球))+p(不打球)*log(p(不打球)))
~~~~

### CART算法

~~~markdown
1. 使用GINI系数来构建决策树并且衡量特征的信息量(作用和信息熵想法)。GINI系数越大，信息量越大。
~~~

$GINI系数公式：\sum_{i=1}^{n}P(C_i)(1-P(C_i))=1-\sum_{i=1}^{n}P(C_i)^2$

### 决策树的问题

~~~markdown
在构建决策树的时候，如果树的规模过大，容易过拟合
~~~

#### 解决方案-剪枝

~~~markdown
如果剪枝，剪的是高度，是限制了特征数
如果剪枝，剪的是宽度，是限制了样本数

1. 预剪枝：在构建树的过程中就进行剪枝
	1. 不容易溢出内存，容易出现剪错枝的情况，进而容易导致欠拟合
2. 后剪枝：树构建完毕之后再剪枝
	1. 内存占用较高，但是不容易出现剪错枝的情况，但是树的规模会很大
~~~

~~~python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
df = pd.read_csv('dating.txt',sep='\t',names=['salary','taobao','tv','result'])
X = df.iloc[:,:3]
Y = df.iloc[:,-1]
scaler = MinMaxScaler()
scaler.fit(X)
X = scaler.transform(X)
train_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size=0.2)
tree = DecisionTreeClassifier(criterion='log_loss')
tree.fit(train_X,train_Y)
print(tree.score(test_X, test_Y))
~~~

### 文本处理

~~~markdown
1. TF-IDF:词频-逆文档频率
	这个词在该文本中出现的次数/改词在文章中出现的总次数
~~~

### 集成算法 融合模型

~~~markdown
1. 
弱训练器：KNN、线性回归、决策树：分类器的效果较差
强训练器：逻辑回归、朴素贝叶斯：分类器的效果还可以
一般来说，强训练器不用做集成算法
集成算法：多个弱分类器放在一起共同使用
核心原理：三个臭皮匠顶一个诸葛亮
集成算法的最终输出结果：少数服从多数
~~~

#### Bagging算法

~~~markdown
并行多个弱训练器，每个弱训练器都会输出一个结果，最终的结果由少数服从多数，投票选出
使用控制变量法：
	1. K的值
	2. 使用部分训练集
使用控制弱训练器法：
	KNN
	线性回归
	决策树
~~~

#### 随机森林

~~~markdown
1. 随机
	1. 随机样本
	2. 随机特征
2. 森林
	多颗决策树

随机抽取一部分样本和一部分特征，构建一个新的子样本集，再利用子样本集构建一个决策树
并行多颗决策树，最终结果由多颗决策树的结果投票选举出

缺点：
	1. 计算量大
优点：
	1. 可以用作特征重要性分析
	2. 不容易过拟合---泛化能力强---鲁棒性强
	
A B C
A+B=60%
B+C=80%
A+C=70%
C-A = 20%
C = 45%
B = 35%
A = 25%
~~~

#### adaboost

~~~markdown
1. 串行多个弱训练器，通过改变参数的更新方式来强迫每个训练器更关注于上个训练器训练错误的数据
~~~

![adaboost](D:\TYUT教学资料\TYUT2022\笔记\.assets\adaboost.png)

#### stacking算法

~~~markdown
工业界基本上不用，主要是竞赛、学术用的多
1. 运行多个弱训练器，将多个弱训练器输出的结果当作新的特征矩阵，再通过一个强训练器，来发现这几个弱训练器预测的结果与真实值之间的关系。
~~~

~~~python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,AdaBoostClassifier
df = pd.read_csv('../day3/dating.txt', sep='\t', names=['salary', 'taobao', 'tv', 'result'])
print(df)
print(type(df))
X = df.iloc[:,:3]
Y = df.iloc[:,-1]
scaler = MinMaxScaler()
scaler.fit(X)
X = scaler.transform(X)

train_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size=0.2)
# model = BaggingClassifier(base_estimator=KNeighborsClassifier(),n_estimators=100)
# model = RandomForestClassifier(n_estimators=100)
model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())
model.fit(train_X,train_Y)
print(model.score(test_X,test_Y))
data = scaler.transform([[90000,23,1]])
print(model.predict(data))
~~~

### SVM支持向量机

~~~markdown
1. 向量的模：向量的模 表示||A||	所有的元素的平方和再开方
2. 计算一个向量到另一个向量的投影长度
~~~

![投影长度的计算](D:\TYUT教学资料\TYUT2022\笔记\.assets\投影长度的计算.png)

#### 含义

~~~markdown
1. 支持向量：影响分割线位置的那些个点
2. 机(Machine)：算法
~~~

![SVM支持向量机](D:\TYUT教学资料\TYUT2022\笔记\.assets\SVM支持向量机.png)

~~~markdown
只要数据是线性可分的，SVM总能找到一个最合适的分割线，如果线性不可分？
低维度不可分，转为高维度可分

高维度了之后，主要是为了确定超平面的位置
~~~

#### 核函数

~~~markdown
两个向量的内积：A=[a1,a2,...,an]   B=[b1,b2,...,bn]
	a1*b1+b2*b2+...+an*bn=A.T*B=A * B

高斯核函数可以用来计算两个向量的相似度，又称为相似度函数
~~~

$高斯核函数=径向基函数:f(x)=e^{-\frac{||x-l||}{2\sigma^2}},x和l都是一个向量,\sigma是一个常数$

![SVM支持向量机多维度](D:\TYUT教学资料\TYUT2022\笔记\.assets\SVM支持向量机多维度.png)

# 作业

$线性回归假设函数：Y_1=w^TX_1+b$

$逻辑回归假设函数:h_\theta(x)=\frac{1}{1+e^{\theta^Tx}}$

$逻辑回归损失函数:J{(\theta)}=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}log(\sigma(X_b^{(i)}\theta))+(1-y^{(i)})log(1-\sigma(X_b^{(i)}\theta)),其中\sigma(t)=\frac{1}{1+e^{(-t)}}$

$均方差函数:MSE=\frac{1}{N}(\hat y-y)^2,N为样本个数$

$对数损失函数:f(x)=-(ylog_2(\hat y)+(1-y)log_2(1-\hat y))$

$信息熵计算公式：H(X)=-\sum_{i=1}^{m}p_ilog_2(p_i)$

$信息增益率：g_r(X,Y)=\frac{g(X,Y)}{H(Y)},其中g(X,Y)=H(Y)-H(Y|X),条件 X,事件 Y $

$GINI系数计算公式：G(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$

$贝叶斯公式：P(A|B)=\frac{P(B|A)P(A)}{P(B)}$

$全概率公式：P(A)=\sum_{i=1}^{n}P(B_i)P(A|B_i)$

$欧氏距离：d=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$

$余弦距离：cos(x,y)=\frac{x·y}{|x|·|y|}=\frac{\sum_{i=1}^{n}x_iy_i}{\sqrt{\sum_{i=1}^{n}x_i^2}\sqrt{\sum_{i=1}^{n}y_i^2}}$















